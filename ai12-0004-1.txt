!standard 2.1(4.1/2)                               16-08-15    AI12-0004-1/02
!class binding interpretation 15-06-13
!status work item 11-11-08
!status received 11-06-20
!priority Low
!difficulty Medium
!subject Normalization and allowed characters for identifiers
!summary

Ada identifiers that are not in Normalization Form KC are illegal.

Ada text not in Normalization Form C is implementation-defined.

[Editor's note: This AI is formatted in UTF-8 rather than the typical ASCII
because the character examples need extended characters.]

!question

(1) 2.1(4.1/2) says that meaning of program text not in Normalization Form KC is
"implementation-defined". An Ada implementation could reject such text, even if
it is only used in characters and/or string literals. But Normalization Form KC
is very strong, including such things as superscript and subscript characters
(so that writing m² rather than m**2 would not be allowed).

The Unicode identifier recommendations included by reference in ISO/IEC
10646:2011 suggest that Normalization Form KC (NFKC) should be applied to
identifiers (only). There is no reason to apply such a form to the remainder of
Ada text, and especially to allow implementations to reject programs that
contain characters not allowed by that form. Should this permission be
restricted to identifiers? (Yes.)

(2) The Unicode identifier recommendations included by reference in ISO/IEC
10646:2011 (this is Annex 31, the current link as of this writing is
http://www.unicode.org/reports/tr31/tr31-25.html) suggests using the character
classes XID_Start and XID_Continue in identifiers. The classes are defined to be
stable, such that future character set standards (Unicode and 10646 now are kept
fairly close together) will only make upwardly compatible changes. That is not
true of character classes like "letter_uppercase", which is what Ada is
currently defined in terms of.

Should we change to use these classes? (No.)

!recommendation

(See summary.)

!wording

Modify 2.1(4.1/3):

The semantics of an Ada program whose text is not in Normalization Form {C}[KC]
(as defined by Clause 21 of ISO/IEC 10646:2011) is implementation defined.

AARM Ramification: In particular, an implementation can reject such program
source. Portable programs should always be written in Normalization Form C.

AARM Reason: Normalization Form C ensures that all source is in a unique
format; it eliminates ambiguities and security issues potentially caused by
source using unusual sequences of characters. Note that WC3 recommends that
all Internet content be in Normalization Form C. We don't require this as
there is a potentially significant cost to checking, and some implementations
may need to be interoperable with tools that product unnormalized text.

[Editor's note: I considered making this Implementation Advice, but that
seemed to defy sensible wording without using Bob's hated word - "reject".
And the above seemed like enough anyway.]

Add after 2.3(4/3):

Legality Rules

An identifier shall only contain characters that might be allowed in
Normalization Form KC (as defined by Clause 21 of ISO/IEC 10646:2011).

AARM Implementation Note:

An implementation can usually detect this during lexical processing. The
code points not allowed are those for which Unicode property NFKC_QC
(Normalization Form KC Quick_Check) has the value No. We say "might be
allowed" so that characters for which the value is Maybe (yes, one of
the possible values is Maybe) are allowed (these are mainly combining marks).
The necessary tables can be found in
http://www.unicode.org/Public/UCD/latest/ucd/DerivedNormalizationProps.txt.
Versions for older Unicode versions can be found on this site as well;
start at http://www.unicode.org/Public/ and find the appropriate version
number.

[Author's question: Should we include an Is_NFKC operation in
Ada.(Wide_)*Characters.Handling? We've made all of the other identifier
operations available at runtime.]

!discussion

For the first question, it's clear that non-identifier text should not be
subject to normalization form KC. Indeed, both TR31 and TR15 are quite clear
that KC should only be applied to limited text (like identifiers), not the
entire program. (TR31 directly mentions string literals and comments.)

Normalization form KC merges characters based on their basic properties;
normalization form C just puts characters into their minimal canonical format.
(Minimal here means that characters that can be written as a sequence or as
a single character are written as the single character.)

Identifier text needs strong normalization so that it can be compared using
simple case folding. Other text, which does not need to use case folding,
should not be modified in that way.

Also, we do not want the set of identifiers allowed by Ada to be dependent on
the implementation, so we don't want the normalization of identifiers to be
implementation-defined. We have to define the normalization used. We believe
that the best solution is to define that all identifiers are normalized (that
is, identifiers that are not normalized are illegal).

The current situation is that Ada implementations might treat some characters
differently in identifiers. This could be a significant portability problem,
one that is unnecessary. Thus we adopt a rule that identifiers need to be
in Normalization Form KC.

Normalizing text is a very expensive and slow operation (it involves an up to 3x
expansion, decomposing characters into their canonical parts, ordering those
parts, and then recomposing them). This is not an operation we want to use
frequently [or implement - author]. Luckily, Unicode has a short-cut available,
the "Quick_Check" properties (one set for each of the four normalization sets
available). These are relatively large tables, but it provides a simple answer
for each code point. It is simple to scan the text and determine the result.

But as usual, here Unicode giveth and Unicode taketh-away. For Normalization
Form D, the Quick_Check consists of Yes and No values; if a No is encountered,
the string is not normalized and an error can be flagged. But for NFC and NFKC
(that is, the useful forms that emphasize the most compact representation),
Quick_Check is a three state value: Yes, No, and Maybe. In the Maybe case, an
implementation is supposed to normalize the string and see if it changes.

Since normalization is very expensive, we don't do that. We rather say
"might be in NFKC", thus allowing all of the Maybe characters. For Unicode 9.0
(current as of this writing), there are only 110 code points that are Maybe in
the NFC_QC and NFKC_QC tables. (These are mostly combining code points.) All
of the remaining code points are Yes or No. (No meaning of course that they can't
appear in normalized text.) There are 1120 No code points
for NFC_QC and a whopping 4792 for NFKC_QC (again, for Unicode 9.0).

(These tables can be found at
http://www.unicode.org/Public/UCD/latest/ucd/DerivedNormalizationProps.txt;
they're about in the middle of the file.)

A detailed discussion of Unicode normalization can be found in
Unicode Annex 15 (current link as of this writing is
http://www.unicode.org/reports/tr15/tr15.html). It's a lot more readable than
the actual Unicode standard text [which I made the mistake of trying to
understand - Author].

We do retain the rule that Ada source code needs to be normalized in order
to be portable; we just use the much weaker Normalization Form C (NFC).
WC3 recommends that all Internet content be normalized to NFC; that
eliminates duplication and security issues that otherwise might arise.
It seems sensible to recommend (but not require) the same of Ada.

===

As noted in the second question, Annex U of ISO/IEC 10646:2011 directly
references the Unicode Annex 31 document. It's fairly clear that this is the
recommendation of the experts in this field, and we should come as close to
those recommendations as possible. (But don't forget AI05-0227-1 about case
folding and upper case conversions -- we should not undo those corrections.)

We, however, don't change to using XId_Start and XID_Continue for two reasons.
First, compabilitity is very important to Ada; using these classes would require
a number of modifications in order to be compatible. (For instance, underscore
is included in XId_Continue.) Moreover, it would require a lot of detailed
analysis to figure out how the identifiers allowed by these classes differ from
those allowed by Ada 2005 and Ada 2012. While identifiers with non-Latin-1
characters aren't very common, they do exist. Secondly, the XId_Start and
XId_Continue classes are very similar to the classes that Ada already uses for
identifiers; they mainly are created from the same sets of characters that Ada
already uses. Thus we have a situation where there is more risk of
incompatibility than there is useful differences.

Additionally, Unicode has now made most properties "stable" unless there is
a clear mistake (they claim to have only found 6 so far), so the worries
about change to the existing classifications is reduced without making any
changes.

If these classes had existed in Unicode 4.0, we surely would have used them,
but the Unicode people were very confused about identifiers at that time (and
we copied some of that confusion, sadly). TR31 straightened out that situation,
but somewhat too late for Ada to use.

It should be noted that we're actually not that far from the current TR31
recommendations. Our rules are essentially the same as "Filtered
Case-Insensitive Identifiers" (that is, simple case folding and requiring
normalization). Also, "Filtered Normalized Identifiers" seems to suggest
allowing "Maybe" characters unconditionally, which is our solution to the
otherwise very expensive implementation. (Unfortunately, TR31 doesn't give
a suggested implementation for Filtered Case-Insensitive Identifiers using
simple case folding, so we had to figure out one.)

!ACATS test

ACATS B-Test(s) should be written to check that characters in NFC but not
in NFKC are not allowed in identifiers, even if they otherwise have the
correct character classes.

!ASIS

Probably no ASIS interface effect.

!appendix

!topic Inconsistency in text normalization form and allowed characters
!reference Ada 2012 RM (Draft 12) 2.1(4.1/2), 2.2(3/2), 2.3(8.c/2), A.3.2(60/3), A.4.6(8/3), AI05-0114-1
!from Howard W. Ludwig 11-06-20
!discussion

2.1(4.1/2) states The semantics of an Ada program whose text is not in
Normalization Form KC (as defined by section 24 of ISO/IEC 10646:2003) is
implementation defined.

This adheres to recommendations for communicating case-insensitive data across a
network in a manner minimizing security issues. However, it raises two
difficulties (the first of which has shades of leap second):

1.

AI-0114-1 indicates a ruling that characters such as the feminine ordinal
indicator (Unicode code point u+00AA), the micro sign (u+00B5), and the
masculine ordinal indicator (u+00BA) are to be regarded as letters (because
Unicode places them in category Ll, lower-case letters) that can be used in
identifiers, but are not to be treated as letters by
Ada.Characters.Handling.Is_Letter. However, Normalization Form KC does not allow
the use of those characters (with ª being replaced by a [u+0061], µ replaced
by μ [u+03BC], and º replaced by o [u+006F]). If a programmer wishes to avoid
implementation-defined behavior, then the use of these (and some other)
characters in identifiers is forbidden. In any case, it would seem that an Ada
compiler, due to Ada’s case insensitivity, must at the very least treat the
identifier ª as identical to the identifiers A and a. In addition, the AARM
2.3(8.c/2) indicates, in contradiction to the NFKC expectation: An identifier
can use any letter defined by ISO-10646:2003, along with several other
categories.

2.

Normalization Form KC forbids the use of superscripted and subscripted digits,
for example. While I understand motivation for this restriction as applied to
identifiers, I really would like to be able to use the full Unicode character
set in comments. For example, if I declare a variable Area and I would like to
indicate in commentary that its units are square meters, I would like to be able
to write it as:

Area: Float := 0.0;  -- m²

This is illegal in NFKC. A tool that converts an arbitrary text file to NFKC
would convert this to:

Area: Float := 0.0;  -- m2

unsuperscripting the 2 so that it is no longer obvious that squaring is intended
to be indicated. I would like to see comments being explicitly exempted from the
Normalization Form KC expectation indicated by 2.1(4.1/2).

Given all these issues, it might be more straightforward to use Normalization
Form C instead of KC in 2.1(4.1/2).

****************************************************************

From: Randy Brukardt
Sent: Friday, July 1, 2011  7:39 PM

>2.1(4.1/2) states The semantics of an Ada program whose text is not in
>Normalization Form KC (as defined by section 24 of ISO/IEC 10646:2003)
>is implementation defined.

>This adheres to recommendations for communicating case-insensitive data
>across a network in a manner minimizing security issues.

My recollection is that this was inserted in order to sort-of follow a Unicode
recommendation. We originally required the compiler to convert the source to
Normalization Form KC before processing. However, there was great concern that
this conversion could make identifiers that look very different (and likely
compare differently in editors) be treated as the same. Of course, the reverse
also could be true. So this rule was added, with the intent that it would leave
it up to implementors as to whether or not the normalization was performed.

[I should point out that those Unicode recommendations were completely rewritten
between Unicode 4 (which is what Ada 2005 uses) and Unicode 5 -- completely
reversing the recommendations in some areas. So I don't know what is recommended
now on this specific topic.]

>...However, it
>raises two difficulties (the first of which has shades of leap second):

>1.
>
>AI-0114-1 indicates a ruling that characters such as the feminine
>ordinal indicator (Unicode code point u+00AA), the micro sign (u+00B5),
>and the masculine ordinal indicator (u+00BA) are to be regarded as
>letters (because Unicode places them in category Ll, lower-case
>letters) that can be used in identifiers, but are not to be treated as letters by
>Ada.Characters.Handling.Is_Letter.
>However, Normalization Form KC does not allow the use of those
>characters (with ª being replaced by a [u+0061], µ replaced by µ [u+03BC], and º replaced by o [u+006F]).
>If a programmer wishes to avoid implementation-defined behavior, then
>the use of these (and some other) characters in identifiers is forbidden.

I agree with this.

>In any case, it would seem that an Ada compiler, due to Ada’s case
>insensitivity, must at the very least treat the identifier ª as identical to the identifiers A and a.

Now I've lost you. If the compiler implements the normalization, then it will
never see ª in an identifier. Otherwise, the letter (if not normalized) is
clearly *not* the same as A (based on the case folding tables). The net effect
is that a compiler could do it either way and be within the requirements of the
language. After all, "implementation-defined" is just that; the compiler could
set off a fireworks show (this being the July 4th weekend) if it wanted to when
the source text is not in Normalization Form KC; pretty much anything goes so
long as it is documented. We probably would have preferred to restrict the
outcomes but that isn't really an option given the framework of the Ada
Standard.

>In addition, the AARM 2.3(8.c/2) indicates, in contradiction to the NFKC expectation:
>An identifier can use any letter defined by ISO-10646:2003, along with several
>other categories.

The AARM, of course, is not normative, and it is discussing the language
ignoring NFKC issues. I'm actually surprised to hear that NFKC eliminates some
characters completely, as that seems like a bad thing. In any case, if the
compiler does not normalize the source input before processing, then the AARM
note is strictly true. (I believe that all existing Ada 2005 compilers don't
normalize.)

>2. Normalization Form KC forbids the use of superscripted and
>subscripted digits, for example. While I understand motivation for this
>restriction as applied to identifiers, I really would like to be able
>to use the full Unicode character set in comments. For example, if I
>declare a variable Area and I would like to indicate in commentary that its
>units are square meters, I would like to be able to write it as:
>
>	Area: Float := 0.0;  -- m²
>
>This is illegal in NFKC. A tool that converts an arbitrary text file to
>NFKC would convert this to:
>
>	Area: Float := 0.0;  -- m2
>
>	unsuperscripting the 2 so that it is no longer obvious that squaring
>is intended to be indicated. I would like to see comments being
>explicitly exempted from the Normalization Form KC expectation indicated by 2.1(4.1/2).

The intent was that a compiler could normalize all of the source, or leave the
source in its original form. In either case, comments would have the same
effect. It would greatly complicate the implementation to turn normalization on
and off in different places, and it would violate the Unicode recommendation
that all source code be intepreted in NFKC.

Unless the implementation does something silly (like the fireworks-generating
compiler noted earlier), the effect of comments would be the same either way. So
I don't think there is any need to avoid any characters in comments.

>Given all these issues, it might be more straightforward to use Normalization Form C instead of KC in 2.1(4.1/2).

If there actually were any issues, it probably would be better to simply drop
any permission for normalization, since it appears to be actively harmful (but
it really isn't that harmful, as noted above). I find eliminating superscripts
and subscripts to be completely changing the meaning of the text (as you note in
your comments), that goes so far beyond normalization that it is simply insane.
Moreover, if we are going to ignore the Unicode recommendation in this area (as
we had to do for identifiers, see AI05-0227-1 for the sad details), we might as
well go all the way and ignore normalization altogether.

In any case, this is too late for Ada 2012; the earliest that the ARG could take
up this issue would be at the November meeting, and the final draft of Ada 2012
should have been approved before that meeting. So this will be an early Ada 2012
AI and possible Binding Interpretation.

****************************************************************

From: Howard W Ludwig
Sent: Tuesday, July 5, 2011  2:26 PM

...
>[I should point out that those Unicode recommendations were completely
>rewritten between Unicode 4 (which is what >Ada 2005 uses) and Unicode
>5 -- completely reversing the recommendations in some areas. So I don't
>know what is >recommended now on this specific topic.]

I understand that the rules for Unicode 5 and Ada 2012 changed from the previous
version and why. The changes seem sensible, and it could get problematic for
Unicode and Ada to diverge unnecessarily. NFKC is a Unicode recommendation for
text interpreted in a case-insensitive manner (NFC for case-sensitive), so I can
see how 2.1(4.1/2) was established, although it does have troubling consequences
(mentioned below) accompanying certain advantages.

...
>>In any case, it would seem that an Ada compiler, due to Ada’s case
>>insensitivity, must at the very least treat the identifier ª as
>>identical to the identifiers A and a.
>
>Now I've lost you. If the compiler implements the normalization, then it will
>never see ª in an identifier. Otherwise, the letter (if not normalized) is
>clearly *not* the same as A (based on the case folding tables).
>The net effect is that a compiler could do it either way and be within the
>requirements of the language. After all, "implementation-defined" is just
>that; the compiler could set off a fireworks show (this being the July 4th
>weekend) if it wanted to when the source text is not in Normalization Form KC;
>pretty much anything goes so long as it is documented. We probably would have
>preferred to restrict the outcomes but that isn't really an option given the
>framework of the Ada Standard.

My point was based on my interpretation of the implementation-dependence of
source code written in other than NFKC [2.1(4.1/2)]. I regarded this as being my
responsibility to write the code in NFKC so the compiler would behave in a
ARM-defined and -compliant manner. If that is the case, then I must not use ª,
µ [u+00B5], and º identifiers in my code, because they are not legal for NFKC.
I see your point that the compiler could do the normalization for me (somewhat
like a preprocessor); however, if that happens, the compiler would replace ª by
a and º by o, effectively making the identifier ª identical to the identifiers
A and a--this is what I was really making reference to in my original statement
where you lost me. The whole point of normalization is to clarify when two
characters are clearly intended to be the same but are written as text with
different code points (for example, å can be written as the simple u+00E5 or as
the compound u+0061 u+030A, for which NFKC requires the former and a conversion
tool u+would recognize the latter and convert it to the former). The different
normalization forms provide different prescriptions as to when two different
sequences of code points are to be treated as the same character. NFC is very
simple and would say, like NFKC, that å should be written as u+00E5 and not as
u+0061 u+030A; NFD and NFKD make the opposite choice. However, NFKC goes way
beyond NFC and makes many more sequences of code points identical, so that font
effects like subscripting, superscripting, bolding, etc. are treated as
insignificant to meaning and only the unadorned character version code point is
to be accepted (ª gets converted to a, for example).

I understand about "implementation-defined" allowing fireworks as long as it is
documented. If I comply with NFKC in my code, I will not use those three
characters myself, even though the ARM explicitly states they are legal in
identifiers; if the compiler does the normalization for me, then it will
eliminate those characters (even though explicitly allowed) and remap them as I
stated before; otherwise, the compiler can do anything (treat ª as the same as
a, treat ª as totally distinct from a, or generate fireworks).  That is why I
try to avoid implementation- dependence, so I don't have to worry about such
variations. In this case, it means I need to write my code as NFKC to avoid one
compiler generating fireworks while another compiler does what I had hoped it
would do. That is why I am making somewhat of a big deal about this.

...
>The intent was that a compiler could normalize all of the source, or leave
>the source in its original form. In >either case, comments would have the same effect.
>It would greatly complicate the implementation to turn
>normalization on and off in different places, and it would violate the
>Unicode recommendation that all source code be interpreted in NFKC.
>
>Unless the implementation does something silly (like the fireworks-generating compiler
>noted earlier), the effect of comments would be the same either way. So I don't think
there is any need to avoid any characters in comments.

If the compiler does the normalization to NFKC and any generated listings are
produced after the normalization, my superscripts would be undone in the
listing. (My understanding is that Annex H, if supported by a compiler, requires
a compiler to be able to generate listings, potentially with generated assembly
language, to enable certain security reviews, and it is wide open how that is
done, so post-normalization listings are quite plausible.)

The other (and to me more important) issue is that, with the
implementation-dependence for non-NFKC source code, the compiler might refuse to
accept non-NFKC text (with or without accompanying fireworks), which would
prevent me from using the superscripts if I want my code compiled, unless there
are explicit ARM constraints forbidding rejecting code solely because comments
are not written in NFKC.

>>Given all these issues, it might be more straightforward to use
Normalization Form C instead of KC in 2.1(4.1/2).
>
>If there actually were any issues, it probably would be better to simply drop
>any permission for normalization, since it appears to be actively harmful (but
>it really isn't that harmful, as noted above). I find eliminating superscripts
>and subscripts to be completely changing the meaning of the text (as you note
>in your comments), that goes so far beyond normalization that it is simply
>insane. Moreover, if we are going to ignore the Unicode recommendation in this
>area (as we had to do for identifiers, see AI05-0227-1 for the sad details),
>we might as well go all the way and ignore normalization altogether.

I agree with your insanity comment, but that is what NFKC and NFKD do. (NFC and
NFD are far more sane in this regard.) However, as I pointed out earlier, the
normalization is useful for letting all parties know how to indicate in a clear,
unique, unambiguous manner a particular character that otherwise has multiple
ways of being expressed, so it seems a little dangerous to throw out the whole
thing. I know that WG9 in general and Robert Dewar have had fun dealing with
case insensitivity in the context of the German ß versus SS and ss, as well as
the Turkish lower-case dotless and dotted i both being written as I in upper
case. Character composition and character compatibility in Unicode just extend
these issues into a broader realm with more cans of worms (which is why I
alluded to the leap second issues that proved difficult to close completely in a
self-consistent manner).

>In any case, this is too late for Ada 2012; the earliest that the ARG could
>take up this issue would be at the November meeting, and the final draft of
>Ada 2012 should have been approved before that meeting. So this will be an early
>Ada 2012 AI and possible Binding Interpretation.

I understand that progress has to be made, and at some point you have to say you
are done for a given phase or stage, and you move on. I do not wish to cause a
delay in releasing Ada 2012. I still appreciate your consideration at some
point, as well as WG9's desire to have Ada becoming an ever better language
without breaking what people may already be using.

****************************************************************
